nohup: ignoring input
W0916 06:48:22.962000 130853294090048 torch/distributed/run.py:779] 
W0916 06:48:22.962000 130853294090048 torch/distributed/run.py:779] *****************************************
W0916 06:48:22.962000 130853294090048 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0916 06:48:22.962000 130853294090048 torch/distributed/run.py:779] *****************************************
Device: cuda
CUDA Version: 12.1
Pytorch 2.4.1+cu121
Device: cudaNum CPUs:
 CUDA Version: 12.1160

Pytorch 2.4.1+cu121
Num GPUs: 2
GPU Type: NVIDIA A100 80GB PCIe
Num CPUs: 160
Num GPUs: 2
GPU Type: NVIDIA A100 80GB PCIe
[2025-09-16 06:48:30,386] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-16 06:48:30,397] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-16 06:48:32,443] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-16 06:48:32,484] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-16 06:48:32,596] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-16 06:48:32,637] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-16 06:48:32,637] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/home/chenchen/anaconda3/envs/judge/lib/python3.11/site-packages/transformers/training_args.py:2093: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
  warnings.warn(
{'__cached__setup_devices': device(type='cuda', index=1),
 '_n_gpu': 1,
 'accelerator_config': AcceleratorConfig(split_batches=False,
                                         dispatch_batches=None,
                                         even_batches=True,
                                         use_seedable_sampler=True,
                                         non_blocking=False,
                                         gradient_accumulation_kwargs=None,
                                         use_configured_state=False),
 'adafactor': False,
 'adam8bit': False,
 'adam_beta1': 0.9,
 'adam_beta2': 0.999,
 'adam_epsilon': 1e-08,
 'auto_find_batch_size': False,
 'average_tokens_across_devices': True,
 'batch_eval_metrics': False,
 'bf16': True,
 'bf16_full_eval': False,
 'bits': 16,
 'cache_dir': './data',
 'data_seed': 42,
 'dataloader_drop_last': False,
 'dataloader_num_workers': 3,
 'dataloader_persistent_workers': False,
 'dataloader_pin_memory': True,
 'dataloader_prefetch_factor': None,
 'dataset_format': 'alpaca',
 'dataset_name_or_path': 'data/ours/train/alpaca_50p_gpt4o_clean.json',
 'ddp_backend': None,
 'ddp_broadcast_buffers': None,
 'ddp_bucket_cap_mb': None,
 'ddp_find_unused_parameters': False,
 'ddp_timeout': 1800,
 'debug': [],
 'deepspeed': 'config/ds_config_zero3.json',
 'deepspeed_plugin': DeepSpeedPlugin(hf_ds_config=<transformers.integrations.deepspeed.HfTrainerDeepSpeedConfig object at 0x7a53d845ff50>,
                                     gradient_accumulation_steps=8,
                                     gradient_clipping=1.0,
                                     zero_stage=3,
                                     is_train_batch_min=True,
                                     offload_optimizer_device='none',
                                     offload_param_device='none',
                                     offload_optimizer_nvme_path='none',
                                     offload_param_nvme_path='none',
                                     zero3_init_flag=True,
                                     zero3_save_16bit_model=False,
                                     transformer_moe_cls_names=None,
                                     enable_msamp=False,
                                     msamp_opt_level='O1'),
 'disable_tqdm': False,
 'distributed_state': Distributed environment: DistributedType.DEEPSPEED  Backend: nccl
Num processes: 2
Process index: 1
Local process index: 1
Device: cuda:1
,
 'do_eval': False,
 'do_predict': False,
 'do_train': True,
 'double_quant': True,
 'eval_accumulation_steps': None,
 'eval_dataset_size': 1024,
 'eval_delay': 0,
 'eval_do_concat_batches': True,
 'eval_on_start': False,
 'eval_steps': None,
 'eval_strategy': <IntervalStrategy.NO: 'no'>,
 'eval_use_gather_object': False,
 'fp16': False,
 'fp16_backend': 'auto',
 'fp16_full_eval': False,
 'fp16_opt_level': 'O1',
 'fsdp': [],
 'fsdp_config': {'min_num_params': 0,
                 'xla': False,
                 'xla_fsdp_grad_ckpt': False,
                 'xla_fsdp_v2': False},
 'fsdp_min_num_params': 0,
 'fsdp_transformer_layer_cls_to_wrap': None,
 'full_determinism': False,
 'full_finetune': True,
 'generation_config': GenerationConfig {
  "max_new_tokens": 256
}
,
 'generation_max_length': None,
 'generation_num_beams': None,
 'gradient_accumulation_steps': 8,
 'gradient_checkpointing': True,
 'gradient_checkpointing_kwargs': None,
 'greater_is_better': None,
 'group_by_length': True,
 'half_precision_backend': 'auto',
 'hf_deepspeed_config': <transformers.integrations.deepspeed.HfTrainerDeepSpeedConfig object at 0x7a53d845ff50>,
 'hub_always_push': False,
 'hub_model_id': None,
 'hub_private_repo': None,
 'hub_revision': None,
 'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>,
 'hub_token': '<PUSH_TO_HUB_TOKEN>',
 'ignore_data_skip': False,
 'include_for_metrics': [],
 'include_inputs_for_metrics': False,
 'include_num_input_tokens_seen': False,
 'include_tokens_per_second': False,
 'jit_mode_eval': False,
 'label_names': None,
 'label_smoothing_factor': 0.0,
 'learning_rate': 2e-05,
 'length_column_name': 'length',
 'liger_kernel_config': None,
 'load_best_model_at_end': False,
 'local_rank': 1,
 'log_level': 'passive',
 'log_level_replica': 'warning',
 'log_on_each_node': True,
 'logging_dir': './output/llama3_full_finetune_clean/runs/Sep16_06-48-30_chenchen',
 'logging_first_step': False,
 'logging_nan_inf_filter': True,
 'logging_steps': 10,
 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>,
 'lora_alpha': 16,
 'lora_dropout': 0.0,
 'lora_r': 64,
 'lr_scheduler_kwargs': {},
 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>,
 'max_grad_norm': 1.0,
 'max_memory_MB': 80000,
 'max_steps': -1,
 'max_train_samples': 50,
 'metric_for_best_model': None,
 'model_name_or_path': 'meta-llama/Llama-3.1-8B',
 'mp_parameters': '',
 'neftune_noise_alpha': None,
 'no_cuda': False,
 'num_train_epochs': 4,
 'optim': <OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>,
 'optim_args': None,
 'optim_target_modules': None,
 'output_dir': './output/llama3_full_finetune_clean',
 'overwrite_output_dir': False,
 'parallelism_config': None,
 'past_index': -1,
 'per_device_eval_batch_size': 8,
 'per_device_train_batch_size': 2,
 'per_gpu_eval_batch_size': None,
 'per_gpu_train_batch_size': None,
 'predict_with_generate': False,
 'prediction_loss_only': False,
 'push_to_hub': False,
 'push_to_hub_model_id': None,
 'push_to_hub_organization': None,
 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>',
 'quant_type': 'nf4',
 'ray_scope': 'last',
 'remove_unused_columns': False,
 'report_to': [],
 'restore_callback_states_from_checkpoint': False,
 'resume_from_checkpoint': None,
 'resume_path': '',
 'run_name': None,
 'save_on_each_node': False,
 'save_only_model': False,
 'save_safetensors': True,
 'save_steps': 250,
 'save_strategy': <SaveStrategy.EPOCH: 'epoch'>,
 'save_total_limit': 1,
 'seed': 42,
 'skip_memory_metrics': True,
 'sortish_sampler': False,
 'source_max_len': 1024,
 'target_max_len': 256,
 'tf32': None,
 'token': None,
 'torch_compile': False,
 'torch_compile_backend': None,
 'torch_compile_mode': None,
 'torch_empty_cache_steps': None,
 'torchdynamo': None,
 'tpu_metrics_debug': False,
 'tpu_num_cores': None,
 'train_on_source': False,
 'trust_remote_code': True,
 'use_cpu': False,
 'use_ipex': False,
 'use_legacy_prediction_loop': False,
 'use_liger_kernel': False,
 'use_mps_device': False,
 'warmup_ratio': 0.03,
 'warmup_steps': 0,
 'weight_decay': 0.01}
loading base model meta-llama/Llama-3.1-8B...
/home/chenchen/anaconda3/envs/judge/lib/python3.11/site-packages/transformers/training_args.py:2093: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
  warnings.warn(
{'__cached__setup_devices': device(type='cuda', index=0),
 '_n_gpu': 1,
 'accelerator_config': AcceleratorConfig(split_batches=False,
                                         dispatch_batches=None,
                                         even_batches=True,
                                         use_seedable_sampler=True,
                                         non_blocking=False,
                                         gradient_accumulation_kwargs=None,
                                         use_configured_state=False),
 'adafactor': False,
 'adam8bit': False,
 'adam_beta1': 0.9,
 'adam_beta2': 0.999,
 'adam_epsilon': 1e-08,
 'auto_find_batch_size': False,
 'average_tokens_across_devices': True,
 'batch_eval_metrics': False,
 'bf16': True,
 'bf16_full_eval': False,
 'bits': 16,
 'cache_dir': './data',
 'data_seed': 42,
 'dataloader_drop_last': False,
 'dataloader_num_workers': 3,
 'dataloader_persistent_workers': False,
 'dataloader_pin_memory': True,
 'dataloader_prefetch_factor': None,
 'dataset_format': 'alpaca',
 'dataset_name_or_path': 'data/ours/train/alpaca_50p_gpt4o_clean.json',
 'ddp_backend': None,
 'ddp_broadcast_buffers': None,
 'ddp_bucket_cap_mb': None,
 'ddp_find_unused_parameters': False,
 'ddp_timeout': 1800,
 'debug': [],
 'deepspeed': 'config/ds_config_zero3.json',
 'deepspeed_plugin': DeepSpeedPlugin(hf_ds_config=<transformers.integrations.deepspeed.HfTrainerDeepSpeedConfig object at 0x7b14baad99d0>,
                                     gradient_accumulation_steps=8,
                                     gradient_clipping=1.0,
                                     zero_stage=3,
                                     is_train_batch_min=True,
                                     offload_optimizer_device='none',
                                     offload_param_device='none',
                                     offload_optimizer_nvme_path='none',
                                     offload_param_nvme_path='none',
                                     zero3_init_flag=True,
                                     zero3_save_16bit_model=False,
                                     transformer_moe_cls_names=None,
                                     enable_msamp=False,
                                     msamp_opt_level='O1'),
 'disable_tqdm': False,
 'distributed_state': Distributed environment: DistributedType.DEEPSPEED  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0
,
 'do_eval': False,
 'do_predict': False,
 'do_train': True,
 'double_quant': True,
 'eval_accumulation_steps': None,
 'eval_dataset_size': 1024,
 'eval_delay': 0,
 'eval_do_concat_batches': True,
 'eval_on_start': False,
 'eval_steps': None,
 'eval_strategy': <IntervalStrategy.NO: 'no'>,
 'eval_use_gather_object': False,
 'fp16': False,
 'fp16_backend': 'auto',
 'fp16_full_eval': False,
 'fp16_opt_level': 'O1',
 'fsdp': [],
 'fsdp_config': {'min_num_params': 0,
                 'xla': False,
                 'xla_fsdp_grad_ckpt': False,
                 'xla_fsdp_v2': False},
 'fsdp_min_num_params': 0,
 'fsdp_transformer_layer_cls_to_wrap': None,
 'full_determinism': False,
 'full_finetune': True,
 'generation_config': GenerationConfig {
  "max_new_tokens": 256
}
,
 'generation_max_length': None,
 'generation_num_beams': None,
 'gradient_accumulation_steps': 8,
 'gradient_checkpointing': True,
 'gradient_checkpointing_kwargs': None,
 'greater_is_better': None,
 'group_by_length': True,
 'half_precision_backend': 'auto',
 'hf_deepspeed_config': <transformers.integrations.deepspeed.HfTrainerDeepSpeedConfig object at 0x7b14baad99d0>,
 'hub_always_push': False,
 'hub_model_id': None,
 'hub_private_repo': None,
 'hub_revision': None,
 'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>,
 'hub_token': '<PUSH_TO_HUB_TOKEN>',
 'ignore_data_skip': False,
 'include_for_metrics': [],
 'include_inputs_for_metrics': False,
 'include_num_input_tokens_seen': False,
 'include_tokens_per_second': False,
 'jit_mode_eval': False,
 'label_names': None,
 'label_smoothing_factor': 0.0,
 'learning_rate': 2e-05,
 'length_column_name': 'length',
 'liger_kernel_config': None,
 'load_best_model_at_end': False,
 'local_rank': 0,
 'log_level': 'passive',
 'log_level_replica': 'warning',
 'log_on_each_node': True,
 'logging_dir': './output/llama3_full_finetune_clean/runs/Sep16_06-48-30_chenchen',
 'logging_first_step': False,
 'logging_nan_inf_filter': True,
 'logging_steps': 10,
 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>,
 'lora_alpha': 16,
 'lora_dropout': 0.0,
 'lora_r': 64,
 'lr_scheduler_kwargs': {},
 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>,
 'max_grad_norm': 1.0,
 'max_memory_MB': 80000,
 'max_steps': -1,
 'max_train_samples': 50,
 'metric_for_best_model': None,
 'model_name_or_path': 'meta-llama/Llama-3.1-8B',
 'mp_parameters': '',
 'neftune_noise_alpha': None,
 'no_cuda': False,
 'num_train_epochs': 4,
 'optim': <OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>,
 'optim_args': None,
 'optim_target_modules': None,
 'output_dir': './output/llama3_full_finetune_clean',
 'overwrite_output_dir': False,
 'parallelism_config': None,
 'past_index': -1,
 'per_device_eval_batch_size': 8,
 'per_device_train_batch_size': 2,
 'per_gpu_eval_batch_size': None,
 'per_gpu_train_batch_size': None,
 'predict_with_generate': False,
 'prediction_loss_only': False,
 'push_to_hub': False,
 'push_to_hub_model_id': None,
 'push_to_hub_organization': None,
 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>',
 'quant_type': 'nf4',
 'ray_scope': 'last',
 'remove_unused_columns': False,
 'report_to': [],
 'restore_callback_states_from_checkpoint': False,
 'resume_from_checkpoint': None,
 'resume_path': '',
 'run_name': None,
 'save_on_each_node': False,
 'save_only_model': False,
 'save_safetensors': True,
 'save_steps': 250,
 'save_strategy': <SaveStrategy.EPOCH: 'epoch'>,
 'save_total_limit': 1,
 'seed': 42,
 'skip_memory_metrics': True,
 'sortish_sampler': False,
 'source_max_len': 1024,
 'target_max_len': 256,
 'tf32': None,
 'token': None,
 'torch_compile': False,
 'torch_compile_backend': None,
 'torch_compile_mode': None,
 'torch_empty_cache_steps': None,
 'torchdynamo': None,
 'tpu_metrics_debug': False,
 'tpu_num_cores': None,
 'train_on_source': False,
 'trust_remote_code': True,
 'use_cpu': False,
 'use_ipex': False,
 'use_legacy_prediction_loop': False,
 'use_liger_kernel': False,
 'use_mps_device': False,
 'warmup_ratio': 0.03,
 'warmup_steps': 0,
 'weight_decay': 0.01}
loading base model meta-llama/Llama-3.1-8B...
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-09-16 06:48:35,526] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 2
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-09-16 06:48:35,543] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 2
[2025-09-16 06:48:36,983] [INFO] [partition_parameters.py:366:__exit__] finished initializing model - num_params = 221, num_elems = 5.80B
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/chenchen/gjx/Judge/train.py", line 731, in <module>
[rank0]:     train()
[rank0]:   File "/home/chenchen/gjx/Judge/train.py", line 645, in train
[rank0]:     model, tokenizer = get_accelerate_model(args, checkpoint_dir)
[rank0]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/chenchen/gjx/Judge/train.py", line 340, in get_accelerate_model
[rank0]:     model = AutoModelForCausalLM.from_pretrained(
[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/chenchen/anaconda3/envs/judge/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 604, in from_pretrained
[rank0]:     return model_class.from_pretrained(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/chenchen/anaconda3/envs/judge/lib/python3.11/site-packages/transformers/modeling_utils.py", line 288, in _wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/chenchen/anaconda3/envs/judge/lib/python3.11/site-packages/transformers/modeling_utils.py", line 5103, in from_pretrained
[rank0]:     model = cls(config, *model_args, **model_kwargs)
[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/chenchen/anaconda3/envs/judge/lib/python3.11/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 529, in wrapper
[rank0]:     f(module, *args, **kwargs)
[rank0]:   File "/home/chenchen/anaconda3/envs/judge/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 420, in __init__
[rank0]:     self.model = LlamaModel(config)
[rank0]:                  ^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/chenchen/anaconda3/envs/judge/lib/python3.11/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 529, in wrapper
[rank0]:     f(module, *args, **kwargs)
[rank0]:   File "/home/chenchen/anaconda3/envs/judge/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 342, in __init__
[rank0]:     [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
[rank0]:   File "/home/chenchen/anaconda3/envs/judge/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 342, in <listcomp>
[rank0]:     [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
[rank0]:      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/chenchen/anaconda3/envs/judge/lib/python3.11/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 529, in wrapper
[rank0]:     f(module, *args, **kwargs)
[rank0]:   File "/home/chenchen/anaconda3/envs/judge/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 275, in __init__
[rank0]:     self.mlp = LlamaMLP(config)
[rank0]:                ^^^^^^^^^^^^^^^^
[rank0]:   File "/home/chenchen/anaconda3/envs/judge/lib/python3.11/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 529, in wrapper
[rank0]:     f(module, *args, **kwargs)
[rank0]:   File "/home/chenchen/anaconda3/envs/judge/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 149, in __init__
[rank0]:     self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)
[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/chenchen/anaconda3/envs/judge/lib/python3.11/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 529, in wrapper
[rank0]:     f(module, *args, **kwargs)
[rank0]:   File "/home/chenchen/anaconda3/envs/judge/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 99, in __init__
[rank0]:     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
[rank0]:                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/chenchen/anaconda3/envs/judge/lib/python3.11/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 255, in wrapped_fn
[rank0]:     tensor: Tensor = fn(*args, **kwargs)
[rank0]:                      ^^^^^^^^^^^^^^^^^^^
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 3.38 MiB is free. Process 1112589 has 10.39 GiB memory in use. Process 1853736 has 31.42 GiB memory in use. Process 1934798 has 31.15 GiB memory in use. Process 1941349 has 6.14 GiB memory in use. Of the allocated memory 5.40 GiB is allocated by PyTorch, and 115.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W916 06:48:37.862355379 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0916 06:48:41.266000 130853294090048 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 152680 closing signal SIGTERM
E0916 06:48:41.637000 130853294090048 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 152679) of binary: /home/chenchen/anaconda3/envs/judge/bin/python3.11
Traceback (most recent call last):
  File "/home/chenchen/anaconda3/envs/judge/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/chenchen/anaconda3/envs/judge/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/chenchen/anaconda3/envs/judge/lib/python3.11/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/chenchen/anaconda3/envs/judge/lib/python3.11/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/chenchen/anaconda3/envs/judge/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenchen/anaconda3/envs/judge/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-09-16_06:48:41
  host      : chenchen
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 152679)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
